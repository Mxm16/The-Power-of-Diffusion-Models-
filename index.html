<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>CS180 Project 5 (Part A) — Diffusion Models</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
    }
    h1, h2, h3 { color: #1b3d8f; }
    figure { text-align: center; }
    figcaption { font-size: 14px; margin-top: 6px; color: #444; }
    .img-row {
      display: flex;
      gap: 20px;
      margin-top: 15px;
      margin-bottom: 35px;
    }
    .img-row img {
      width: 300px;
      border: 1px solid #ccc;
      border-radius: 6px;
    }
    .section { margin-bottom: 60px; }
  </style>
</head>

<body>

<h1>CS180: Introduction to Computer Vision<br>Project 5 (Part A): Diffusion Models</h1>

<hr>

<!-- ================================================================ -->
<h2>Part 0 — Setup</h2>

<h3>Random Seed</h3>
<p>The random seed used for all image generations is <strong>100</strong>.</p>

<h3>Prompt Embeddings I Generated</h3>
<p>I generated embeddings for the following prompts:</p>
<ul>
  <li><strong>"an oil painting of a snowy mountain village"</strong></li>
  <li><strong>"a photo of a dog"</strong></li>
  <li><strong>"a rocket ship"</strong></li>
</ul>

<h3>Generated Images (20 Inference Steps)</h3>
<div class="img-row">
  <figure><img src="img/prompt_snowyvillage.png"><figcaption>Snowy Mountain Village</figcaption></figure>
  <figure><img src="img/prompt_dog.png"><figcaption>Photo of a Dog</figcaption></figure>
  <figure><img src="img/prompt_rocket.png"><figcaption>Rocket Ship</figcaption></figure>
</div>

<h3>Generated Images (50 Inference Steps)</h3>
<div class="img-row">
  <figure><img src="img/prompt_snowyvillage1.png"><figcaption>Snowy Mountain Village</figcaption></figure>
  <figure><img src="img/prompt_dog1.png"><figcaption>Photo of a Dog</figcaption></figure>
  <figure><img src="img/prompt_rocket1.png"><figcaption>Rocket Ship</figcaption></figure>
</div>

<h3>Reflection</h3>
<p>
The model produced recognizable samples for all three prompts.  
The snowy mountain village images consistently captured an oil-painting texture.  
The dog images were natural-looking, though lower inference steps resulted in slightly more noise.  
The rocket ship prompt showed the clearest difference:  
20 inference steps produced blurrier edges and artifacts, while 50 steps resulted in cleaner structure and better-defined shapes.  
This confirms that higher inference steps significantly improve coherence and detail.
</p>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.1 — Forward Process</h2>

<p>I implemented the forward process:</p>

<pre>NoisyImage_t = √(α̅_t) * image + √(1 - α̅_t) * noise</pre>

<p>Below are noisy Campanile images at various timesteps:</p>

<div class="img-row">
  <figure><img src="img/camp_t250.png"><figcaption>t = 250</figcaption></figure>
  <figure><img src="img/camp_t500.png"><figcaption>t = 500</figcaption></figure>
  <figure><img src="img/camp_t750.png"><figcaption>t = 750</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.2 — Gaussian Blur Denoising</h2>

<p>I applied <code>torchvision.transforms.functional.gaussian_blur</code> to the noisy images.
Gaussian blur reduces high-frequency noise but fails to restore structure.</p>

<figure><img src="img/blur_250.png"><figcaption>Blurred (t=250)</figcaption></figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.3 — One-Step Denoising</h2>

<p>I passed the noisy images through <code>stage_1.unet</code> and removed noise using equation A.2.  
The model produces reasonable reconstructions for low-noise inputs, but performance degrades at t=750.</p>

<div class="img-row">
  <figure><img src="img/onestep_250.png"><figcaption>One-Step (t=250)</figcaption></figure>
  <figure><img src="img/onestep_500.png"><figcaption>One-Step (t=500)</figcaption></figure>
  <figure><img src="img/onestep_750.png"><figcaption>One-Step (t=750)</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.4 — Iterative Denoising</h2>

<p>I constructed a strided timestep schedule:</p>
<pre>[990, 960, 930, ..., 0]</pre>

<p>My iterative denoising implementation uses the reverse diffusion formula and <code>add_variance</code> as required.</p>

<h3>Denoising Progress</h3>
  <figure><img src="img/iter_0.png"><figcaption>>Denoising Progress</figcaption></figure>



<!-- ================================================================ -->
<div class="section">
<h2>Part 1.5 — Sampling from Pure Noise</h2>
<p>Using <code>i_start = 0</code>, I sampled 5 images with the prompt “a high quality photo”.</p>

<div class="img-row">
  <figure><img src="img/sample1.png"><figcaption>Sample 1</figcaption></figure>
  <figure><img src="img/sample2.png"><figcaption>Sample 2</figcaption></figure>
  <figure><img src="img/sample3.png"><figcaption>Sample 3</figcaption></figure>
</div>

<div class="img-row">
  <figure><img src="img/sample4.png"><figcaption>Sample 4</figcaption></figure>
  <figure><img src="img/sample5.png"><figcaption>Sample 5</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.6 — Classifier-Free Guidance</h2>

<p>I implemented CFG using conditional and unconditional UNet predictions.  
CFG scale = 7 significantly improves sampling quality.</p>

<div class="img-row">
  <figure><img src="img/cfg1.png"><figcaption>CFG Sample 1</figcaption></figure>
  <figure><img src="img/cfg2.png"><figcaption>CFG Sample 2</figcaption></figure>
  <figure><img src="img/cfg3.png"><figcaption>CFG Sample 3</figcaption></figure>
</div>
<div class="img-row">
  <figure><img src="img/cfg4.png"><figcaption>CFG Sample 4</figcaption></figure>
  <figure><img src="img/cfg5.png"><figcaption>CFG Sample 5</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7 — SDEdit: Image-to-Image Projection</h2>

<p>I used the prompt <strong>"an oil painting of a snowy mountain village"</strong> for text-guided SDEdit.  
Edits grow stronger as noise increases.</p>

  <figure><img src="img/sdedit_1.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/sdedit_2.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/sdedit_3.png"><figcaption>i_start=1-20</figcaption></figure>



<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.1 — Editing Web Images & Hand-Drawn Images</h2>

<h3>Web Image Example</h3>

  <figure><img src="img/web_1.png"><figcaption>i_start=1-20</figcaption></figure>


<h3>Hand-Drawn Examples</h3>

  <figure><img src="img/draw1_1.png"><figcaption>Drawn Image 1 (i=1-20)</figcaption></figure>
  <figure><img src="img/draw1_20.png"><figcaption>Drawn Image</figcaption></figure>



  <figure><img src="img/draw2_1.png"><figcaption>Drawn Image 2 (i=1-20)</figcaption></figure>
  <figure><img src="img/draw2_20.png"><figcaption>Drawn Image(i=20)</figcaption></figure>

</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.2 — Inpainting</h2>

<p>I implemented inpainting by overwriting unmasked pixels with noised versions of the original image at every step.</p>

<h3>Mask</h3>
<figure><img src="img/mask.png"><figcaption>Mask</figcaption></figure>

<h3>Inpainting Results</h3>
<div class="img-row">
  <figure><img src="img/inpaint1.png"><figcaption>Result 1</figcaption></figure>
  <figure><img src="img/inpaint2.png"><figcaption>Result 2</figcaption></figure>
  <figure><img src="img/inpaint3.png"><figcaption>Result 3</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.3 — Text-Conditional Image-to-Image Translation</h2>

<p>I used the prompt <strong>"an oil painting of a snowy mountain village"</strong> for text-guided projection.</p>

<div class="img-row">
  <figure><img src="img/txt_1.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_2.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_3.png"><figcaption>i_start=1-20</figcaption></figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.8 — Visual Anagrams</h2>

<p>I implemented visual anagrams using two prompts:</p>

<ul>
  <li><strong>"an oil painting of an old man"</strong> (upright)</li>
  <li><strong>"an oil painting of people around a campfire"</strong> (flipped)</li>
</ul>

<div class="img-row">
  <figure><img src="img/anagram1_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram1_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>

<ul>
  <li><strong>"a pencil"</strong> (upright)</li>
  <li><strong>"a rocket ship"</strong> (flipped)</li>
</ul>
  
<div class="img-row">
  <figure><img src="img/anagram2_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram2_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.9 — Hybrid Images</h2>

<p>I implemented hybrid images using:</p>
<pre>ε = lowpass(ε₁) + highpass(ε₂)</pre>

<p>The low-pass filter is Gaussian blur (kernel_size=33, sigma=2).</p>

<h3>Hybrid Image 1</h3>
<div class="img-row">
  <figure><img src="img/hybrid1.png"><figcaption>Hybrid 1</figcaption></figure>
</div>

<h3>Hybrid Image 2</h3>
<div class="img-row">
  <figure><img src="img/hybrid2.png"><figcaption>Hybrid 2</figcaption></figure>
</div>

</div>

<hr>
<p>End of CS180 Project 5A</p>

<hr>

<!-- ================================================================ -->
<h1>Project 5 (Part B): Flow Matching from Scratch</h1>

<div class="section">
<h2>Overview</h2>
<p>
In this part, we train our own flow-matching generative model on the MNIST dataset.
Unlike Part A, where diffusion models were used for image synthesis, here we implement
and train UNets from scratch to perform denoising and generation using flow matching.
</p>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1 — Single-Step Denoising UNet</h2>

<h3>UNet Architecture</h3>
<p>
We implement an unconditional UNet consisting of downsampling and upsampling blocks
with skip connections. The network is trained using an L2 loss to map noisy images
directly to clean MNIST digits.
</p>

<h3>Noising Process Visualization</h3>
<figure>
  <img src="img/part1_noise.png">
  <figcaption>Visualization of the noising process for increasing noise levels</figcaption>
</figure>

<h3>Training Loss</h3>
<figure>
  <img src="img/part1_loss.png">
  <figcaption>Training loss curve for single-step denoising UNet</figcaption>
</figure>

<h3>Denoising Results</h3>
  <figure><img src="img/part1_epoch1.png"><figcaption>Epoch 1,5</figcaption></figure>


<h3>Out-of-Distribution Noise Levels</h3>
<figure>
  <img src="img/part1_ood.png">
  <figcaption>Denoising results on unseen noise levels</figcaption>
</figure>

<h3>Denoising Pure Noise</h3>
<div class="img-row">
  <figure><img src="img/part1_pure_epoch1.png"><figcaption>Epoch 1</figcaption></figure>
  <figure><img src="img/part1_pure_epoch5.png"><figcaption>Epoch 5</figcaption></figure>
</div>
  <img src="img/part1_loss1.png">
<p>
When trained with an MSE loss on pure noise, the network converges to predicting the
average of the training data distribution. As a result, the generated outputs appear
as blurry digit-like patterns resembling the centroid of MNIST digits.
</p>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 2 — Flow Matching</h2>

<h3>Time-Conditioned UNet</h3>
<p>
To enable iterative denoising, we extend the UNet with time conditioning.
A scalar timestep <code>t ∈ [0,1]</code> is injected into the UNet via fully-connected
conditioning blocks (FCBlocks), allowing the model to learn the flow field from noise
to clean images.
</p>

<h3>Training Loss</h3>
<figure>
  <img src="img/part2_time_loss.png">
  <figcaption>Training loss curve for time-conditioned UNet</figcaption>
</figure>

<h3>Sampling Results</h3>
<div class="img-row">
  <figure><img src="img/part2_time_epoch1.png"><figcaption>Epoch 1</figcaption></figure>
  <figure><img src="img/part2_time_epoch5.png"><figcaption>Epoch 5</figcaption></figure>
  <figure><img src="img/part2_time_epoch10.png"><figcaption>Epoch 10</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Class-Conditioned Flow Matching</h2>

<p>
To improve convergence speed and generation quality, we further condition the UNet
on digit class labels using one-hot encodings. Classifier-free guidance is employed
by randomly dropping the class-conditioning signal during training.
</p>

<h3>Training Loss</h3>
<figure>
  <img src="img/part2_class_loss.png">
  <figcaption>Training loss curve for class-conditioned UNet</figcaption>
</figure>

<h3>Sampling Results (Epoch 1, 5, 10)</h3>
<figure>
  <img src="img/part2_class_epoch1.png">
  <img src="img/part2_class_epoch5.png">
  <img src="img/part2_class_epoch10.png">
  <figcaption>
    Class-conditioned samples.
    Each row corresponds to one digit (0–9) with four samples per class.
  </figcaption>
</figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Removing the Learning Rate Scheduler</h2>

<p>
We remove the exponential learning rate scheduler to simplify training.
To compensate for the lack of learning rate decay, we reduce the learning
rate from <code>1e-2</code> to <code>1e-3</code>.
</p>

<p>
This modification leads to stable training and comparable convergence.
The visual quality of generated samples remains similar to models trained
with a scheduler.
</p>

<figure>
  <img src="img/part2_no_sched_epoch10.png">
  <figcaption>Class-conditioned samples trained without learning rate scheduler</figcaption>
</figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Conclusion</h2>

<p>
In this project, we explored denoising and generative modeling using flow matching.
Time conditioning enables iterative generation, while class conditioning significantly
improves convergence speed and controllability. These experiments demonstrate the
effectiveness of flow matching as a principled alternative to diffusion models.
</p>
</div>

<hr>
<p><strong>End of CS180 Project 5 – Part B</strong></p>

</body>
</html>

