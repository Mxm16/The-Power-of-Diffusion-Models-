<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>CS180 Project 5 (Part A) — Diffusion Models</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
    }
    h1, h2, h3 { color: #1b3d8f; }
    figure { text-align: center; }
    figcaption { font-size: 14px; margin-top: 6px; color: #444; }
    .img-row {
      display: flex;
      gap: 20px;
      margin-top: 15px;
      margin-bottom: 35px;
    }
    .img-row img {
      width: 300px;
      border: 1px solid #ccc;
      border-radius: 6px;
    }
    .section { margin-bottom: 60px; }
  </style>
</head>

<body>

<h1>CS180: Introduction to Computer Vision<br>Project 5 (Part A): Diffusion Models</h1>

<hr>
<h2>Part 0 — Setup</h2>

<h3>Random Seed</h3>
<p>The random seed used for all image generations is <strong>100</strong>.</p>

<h3>Prompt Embeddings I Generated</h3>
<p>I generated embeddings for the following prompts:</p>
<ul>
  <li><strong>"an oil painting of a snowy mountain village"</strong></li>
  <li><strong>"a photo of a dog"</strong></li>
  <li><strong>"a rocket ship"</strong></li>
</ul>

<h3>Generated Images (20 Inference Steps)</h3>
<div class="img-row">
  <figure><img src="img/prompt_snowyvillage.png"><figcaption>Snowy Mountain Village</figcaption></figure>
  <figure><img src="img/prompt_dog.png"><figcaption>Photo of a Dog</figcaption></figure>
  <figure><img src="img/prompt_rocket.png"><figcaption>Rocket Ship</figcaption></figure>
</div>

<h3>Generated Images (50 Inference Steps)</h3>
<div class="img-row">
  <figure><img src="img/prompt_snowyvillage1.png"><figcaption>Snowy Mountain Village</figcaption></figure>
  <figure><img src="img/prompt_dog1.png"><figcaption>Photo of a Dog</figcaption></figure>
  <figure><img src="img/prompt_rocket1.png"><figcaption>Rocket Ship</figcaption></figure>
</div>

<h3>Reflection</h3>
<p>
The model produced recognizable samples for all three prompts.  
The snowy mountain village images consistently captured an oil-painting texture.  
The dog images were natural-looking, although lower inference steps resulted in slightly more noise.  
The rocket ship prompt showed the clearest difference:  
20 inference steps produced blurrier edges and artifacts, while 50 steps resulted in cleaner structure and better-defined shapes.  
This confirms that higher inference steps significantly improve coherence and detail.
</p>

<hr>
<div class="section">
<h2>Part 1.1 — Forward Process</h2>

<p><strong>Approach & Function Explanation:</strong> The <code>forward(im, t)</code> function adds noise to a clean image <code>im</code> at timestep <code>t</code>. The formula is:<br>
<code>NoisyImage_t = sqrt(alpha_bar_t) * image + sqrt(1 - alpha_bar_t) * noise</code><br>
where <code>alpha_bar_t</code> comes from <code>alphas_cumprod[t]</code>.  
Noise is generated using <code>torch.randn_like(im)</code> to ensure randomness.</p>

<p>Campanile images at different timesteps:</p>

<div class="img-row">
  <figure><img src="img/camp_t250.png"><figcaption>t = 250</figcaption></figure>
  <figure><img src="img/camp_t500.png"><figcaption>t = 500</figcaption></figure>
  <figure><img src="img/camp_t750.png"><figcaption>t = 750</figcaption></figure>
</div>
</div>

<hr>
<div class="section">
<h2>Part 1.2 — Gaussian Blur Denoising</h2>

<p><strong>Approach & Function Explanation:</strong> We use <code>torchvision.transforms.functional.gaussian_blur</code> to denoise the noisy image.  
The function <code>gaussian_blur(image, kernel_size, sigma)</code> removes high-frequency noise but cannot recover overall structure, so the effect is limited.</p>

<figure><img src="img/blur_250.png"><figcaption>Blurred (t=250)</figcaption></figure>
</div>

<hr>
<div class="section">
<h2>Part 1.3 — One-Step Denoising</h2>

<p><strong>Approach & Function Explanation:</strong> The <code>one_step_denoise(im_noisy, t, prompt_embeds)</code> function uses a pretrained UNet to predict noise <code>ε_hat</code> and then estimates the original image <code>x0_hat</code> using formula A.2:<br>
<code>x0_hat = (im_noisy - sqrt(1-α̅_t) * ε_hat) / sqrt(α̅_t)</code><br>
This method works well at low noise, but performance drops with higher noise.</p>

<div class="img-row">
  <figure><img src="img/onestep_250.png"><figcaption>One-Step (t=250)</figcaption></figure>
  <figure><img src="img/onestep_500.png"><figcaption>One-Step (t=500)</figcaption></figure>
  <figure><img src="img/onestep_750.png"><figcaption>One-Step (t=750)</figcaption></figure>
</div>
</div>

<hr>
<div class="section">
<h2>Part 1.4 — Iterative Denoising</h2>

<p><strong>Approach & Function Explanation:</strong> The <code>iterative_denoise(im_noisy, i_start)</code> function denoises the image iteratively from the corresponding timestep.  
At each step, we compute:</p>
<pre>
x_{t'} = sqrt(alpha_bar_{t'}) * x0_hat + sqrt(1 - alpha_bar_{t'}) * ε + add_variance(...)
</pre>
<p>Here, <code>x0_hat</code> is the one-step estimate, and <code>add_variance</code> adds predicted noise variance to maintain diversity in generated samples.</p>

<h3>Denoising Progress</h3>
<figure><img src="img/iter_0.png"><figcaption>Denoising Progress</figcaption></figure>
</div>

<hr>
<div class="section">
<h2>Part 1.5 — Sampling from Pure Noise</h2>

<p><strong>Approach & Function Explanation:</strong> Using <code>iterative_denoise(im_noisy, i_start=0)</code> we can sample images starting from pure random noise, effectively mapping noise to natural image distribution.</p>

<div class="img-row">
  <figure><img src="img/sample1.png"><figcaption>Sample 1</figcaption></figure>
  <figure><img src="img/sample2.png"><figcaption>Sample 2</figcaption></figure>
  <figure><img src="img/sample3.png"><figcaption>Sample 3</figcaption></figure>
</div>
<div class="img-row">
  <figure><img src="img/sample4.png"><figcaption>Sample 4</figcaption></figure>
  <figure><img src="img/sample5.png"><figcaption>Sample 5</figcaption></figure>
</div>
</div>

<hr>
<div class="section">
<h2>Part 1.6 — Classifier-Free Guidance (CFG)</h2>

<p><strong>Approach & Function Explanation:</strong> The <code>iterative_denoise_cfg(im_noisy, i_start, prompt_embeds, scale)</code> function computes both conditional and unconditional noise predictions, then combines them:</p>
<pre>
ε = ε_uncond + scale * (ε_cond - ε_uncond)
</pre>
<p>This amplifies the conditional signal, improving details and structure compared to un-guided sampling.</p>

<div class="img-row">
  <figure><img src="img/cfg1.png"><figcaption>CFG Sample 1</figcaption></figure>
  <figure><img src="img/cfg2.png"><figcaption>CFG Sample 2</figcaption></figure>
  <figure><img src="img/cfg3.png"><figcaption>CFG Sample 3</figcaption></figure>
</div>
<div class="img-row">
  <figure><img src="img/cfg4.png"><figcaption>CFG Sample 4</figcaption></figure>
  <figure><img src="img/cfg5.png"><figcaption>CFG Sample 5</figcaption></figure>
</div>
</div>

<hr>
<div class="section">
<h2>Part 1.7 — SDEdit: Image-to-Image Projection</h2>

<p><strong>Approach & Function Explanation:</strong> The <code>sdedit(im, i_start, prompt_embeds)</code> function adds noise to an input image and then iteratively denoises it.  
Higher noise results in stronger edits. Text guidance can be added with CFG to perform style transfer or naturalization.</p>

<div class="img-row">
  <figure><img src="img/sdedit_1.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/sdedit_2.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/sdedit_3.png"><figcaption>i_start=1-20</figcaption></figure>
</div>
</div>

<hr>
<div class="section">
<h2>Part 1.7.1 — Editing Web Images & Hand-Drawn Images</h2>

<p><strong>Approach:</strong> Non-photorealistic images are projected onto the natural image manifold using noise addition and iterative denoising. The function is the same as SDEdit; only the input image changes.</p>

<h3>Web Image Example</h3>
<figure><img src="img/web_1.png"><figcaption>i_start=1-20</figcaption></figure>

<h3>Hand-Drawn Examples</h3>
<figure><img src="img/draw1_1.png"><figcaption>Drawn Image 1 (i=1-20)</figcaption></figure>
<figure><img src="img/draw1_20.png"><figcaption>Drawn Image</figcaption></figure>
<figure><img src="img/draw2_1.png"><figcaption>Drawn Image 2 (i=1-20)</figcaption></figure>
<figure><img src="img/draw2_20.png"><figcaption>Drawn Image (i=20)</figcaption></figure>
</div>

<hr>
<div class="section">
<h2>Part 1.7.2 — Inpainting</h2>

<p><strong>Approach & Function Explanation:</strong> The <code>inpaint(im, mask, i_start, prompt_embeds)</code> function performs iterative denoising while keeping unmasked regions from the original noisy image.  
This ensures that only masked areas are regenerated. It is essentially <code>iterative_denoise_cfg</code> with a mask constraint.</p>

<h3>Mask</h3>
<figure><img src="img/mask.png"><figcaption>Mask</figcaption></figure>

<h3>Inpainting Results</h3>
<div class="img-row">
  <figure><img src="img/inpaint1.png"><figcaption>Result 1</figcaption></figure>
  <figure><img src="img/inpaint2.png"><figcaption>Result 2</figcaption></figure>
  <figure><img src="img/inpaint3.png"><figcaption>Result 3</figcaption></figure>
</div>
</div>

<hr>
<div class="section">
<h2>Part 1.7.3 — Text-Conditional Image-to-Image Translation</h2>

<p><strong>Approach:</strong> SDEdit guided by text prompts to control style and content of the image. The function is the same as SDEdit, only the prompt changes.</p>

<div class="img-row">
  <figure><img src="img/txt_1.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_2.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_3.png"><figcaption>i_start=1-20</figcaption></figure>
</div>
</div>

<hr>
<div class="section">
<h2>Part 1.8 — Visual Anagrams</h2>

<p><strong>Approach & Function Explanation:</strong> The <code>visual_anagrams(im, prompt_up, prompt_flip)</code> function computes noise predictions for both upright and flipped images, averages them, and generates a perceptual illusion. Implementation is based on iterative denoising with CFG.</p>

<div class="img-row">
  <figure><img src="img/anagram1_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram1_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>

<div class="img-row">
  <figure><img src="img/anagram2_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram2_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>
</div>

<hr>
<div class="section">
<h2>Part 1.9 — Hybrid Images</h2>

<p><strong>Approach & Function Explanation:</strong> The <code>make_hybrids(im, prompt1, prompt2)</code> function combines two prompts by taking low-frequency noise from one and high-frequency noise from the other.  
Low-frequency is extracted using Gaussian Blur (kernel_size=33, sigma=2), high-frequency = original - low-frequency. Implementation is similar to iterative denoising.</p>

<h3>Hybrid Image 1</h3>
<div class="img-row">
  <figure><img src="img/hybrid1.png"><figcaption>Hybrid 1</figcaption></figure>
</div>

<h3>Hybrid Image 2</h3>
<div class="img-row">
  <figure><img src="img/hybrid2.png"><figcaption>Hybrid 2</figcaption></figure>
</div>
</div>

<hr>
<p>End of CS180 Project 5A</p>
<hr>

<!-- ================================================================ -->
<h1>Project 5 (Part B): Flow Matching from Scratch</h1>

<!-- ================================================================ -->
<div class="section">
<h2>Overview</h2>
<p>
In this part, we train flow-matching generative models from scratch on the MNIST dataset.
We first explore the limitations of single-step denoising, and then introduce flow
matching with time and class conditioning to enable iterative image generation.
</p>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1 — Training a Single-Step Denoising UNet</h2>

<p>
We begin by training a simple one-step denoiser. Given a noisy MNIST image, the goal is
to train a UNet that directly maps it to a clean image using an L2 loss.
</p>

<!-- ======================= 1.1 ======================= -->
<h3>1.1 Implementing the UNet</h3>
<p>
The denoiser is implemented as an unconditional UNet consisting of downsampling and
upsampling blocks with skip connections. The network preserves spatial structure while
progressively transforming feature representations.
</p>

<!-- ======================= 1.2 ======================= -->
<h3>1.2 Using the UNet to Train a Denoiser</h3>

<p>
Training pairs are generated by adding Gaussian noise to clean MNIST digits.
Given a clean image x and noise level σ, the noisy image is constructed as:
</p>

<pre>
x_noisy = x + σ · ε,   ε ~ N(0, I)
</pre>
<figure>
  <img src="img/part1_noise.png">
  <figcaption>
    Visualization of the noising process for noise levels
    [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
  </figcaption>
</figure>

<h4>1.2.1 Training</h4>
<p>
<b>Implementation idea:</b>  
For each batch, sample clean images and generate new noisy images on-the-fly using a random σ. 
Feed the noisy batch through the UNet, compute L2 loss with respect to the clean images, 
and backpropagate to update weights. Repeat for multiple epochs so the model learns to 
denoise a range of noise levels.
</p>

<figure>
  <img src="img/part1_loss.png">
  <figcaption>Training loss curve for single-step denoising UNet</figcaption>
</figure>

<figure>
  <img src="img/part1_epoch1.png">
  <figcaption>Sample denoising results after Epoch 1 and Epoch 5</figcaption>
</figure>

<h4>1.2.2 Out-of-Distribution Testing</h4>
<p>
<b>Implementation idea:</b>  
After training, feed noisy images with σ values not seen during training through the trained UNet. 
This tests whether the network generalizes to noise levels outside the training distribution.
</p>

<figure>
  <img src="img/part1_ood.png">
  <figcaption>Denoising results on unseen noise levels</figcaption>
</figure>

<h4>1.2.3 Denoising Pure Noise</h4>
<p>
<b>Implementation idea:</b>  
Replace clean inputs with pure Gaussian noise. Train the UNet to predict the expected clean image from noise. 
This simulates unconditional generation. Since MSE minimizes squared distance, the model will converge 
to the mean of all training images, explaining why outputs are blurry and similar.
</p>

<div class="img-row">
  <figure><img src="img/part1_pure_epoch1.png"><figcaption>Epoch 1</figcaption></figure>
  <figure><img src="img/part1_pure_epoch5.png"><figcaption>Epoch 5</figcaption></figure>
</div>

<figure>
  <img src="img/part1_loss1.png">
  <figcaption>Training loss while denoising pure noise</figcaption>
</figure>

<pre>
y* = argmin_y E[(y - x_digit)^2]
</pre>

</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 2 — Training a Flow Matching Model</h2>

<p>
Single-step denoising is insufficient for generation. We instead use flow matching,
which enables iterative denoising by learning a continuous vector field that transports
noise to the data distribution.
</p>

<h3>2.1 Time-Conditioned UNet</h3>
<p>
<b>Implementation idea:</b>  
Introduce scalar timestep t ∈ [0,1] and embed it via FCBlocks into intermediate layers. 
This lets the UNet learn a flow that depends on the current timestep, enabling smooth interpolation 
from noise to clean images.
</p>

<h3>2.2 Training the Time-Conditioned UNet</h3>
<p>
<b>Implementation idea:</b>  
For each batch: sample a clean image x and a random timestep t.  
Construct the noisy input x_t by interpolating between x and noise.  
Predict the flow field v(x_t, t) with the UNet.  
Compute L2 loss between predicted flow and true flow (x - x_t)/dt.  
Backpropagate and update the UNet weights. Repeat for all batches across multiple epochs.
</p>

<figure>
  <img src="img/part2_time_loss.png">
  <figcaption>Training loss curve for time-conditioned UNet</figcaption>
</figure>

<h3>2.3 Sampling from the Time-Conditioned UNet</h3>
<p>
<b>Implementation idea:</b>  
Start from pure Gaussian noise. At each timestep, feed current image and timestep into the UNet to predict the flow.  
Update the image along the flow direction. Iterate through all timesteps from 0 to 1.  
As training progresses, the network gradually transforms noise into recognizable digits.
</p>

<div class="img-row">
  <figure><img src="img/part2_time_epoch1.png"><figcaption>Epoch 1</figcaption></figure>
  <figure><img src="img/part2_time_epoch5.png"><figcaption>Epoch 5</figcaption></figure>
  <figure><img src="img/part2_time_epoch10.png"><figcaption>Epoch 10</figcaption></figure>
</div>

<h3>2.4 Adding Class Conditioning</h3>
<p>
<b>Implementation idea:</b>  
Introduce a one-hot class vector c for digit labels. Embed c via FCBlocks and inject into UNet layers.  
Randomly drop c 10% of the time for classifier-free guidance, enabling both conditional and unconditional generation.
</p>

<h3>2.5 Training the Class-Conditioned UNet</h3>
<p>
<b>Implementation idea:</b>  
Same as time-conditioned training, but concatenate or add the class-conditioning embedding to intermediate layers.  
Loss remains L2 between predicted and true flow. Random dropout of class embeddings ensures the model can also generate unconditionally.
</p>

<figure>
  <img src="img/part2_class_loss.png">
  <figcaption>Training loss curve for class-conditioned UNet</figcaption>
</figure>

<h3>2.6 Sampling with Class Conditioning</h3>
<p>
<b>Implementation idea:</b>  
Start from noise. At each timestep, provide both the current image and the one-hot class vector to the UNet to predict flow.  
Update the image along the flow. Repeat for all timesteps to generate digit-specific images.  
Use classifier-free guidance to improve sample quality.
</p>

<figure>
  <img src="img/part2_class_epoch1.png">
  <img src="img/part2_class_epoch5.png">
  <img src="img/part2_class_epoch10.png">
  <figcaption>
    Class-conditioned samples.
    Each row corresponds to one digit (0–9) with four samples per class.
  </figcaption>
</figure>

<h3>Removing the Learning Rate Scheduler</h3>
<p>
<b>Implementation idea:</b>  
Removed the exponential decay scheduler for simplicity.  
Compensate by lowering the fixed learning rate (1e-2 → 1e-3) and monitor training.  
Training remains stable and convergence is similar.
</p>

<figure>
  <img src="img/part2_no_sched_loss.png">
  <figcaption>Training loss without a learning rate scheduler</figcaption>
</figure>

<figure>
  <img src="img/part2_no_sched_epoch1.png">
  <img src="img/part2_no_sched_epoch5.png">
  <img src="img/part2_no_sched_epoch10.png">
  <figcaption>
    Class-conditioned samples after training without a learning rate scheduler.
  </figcaption>
</figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Conclusion</h2>

<p>
We showed that single-step denoising collapses to the dataset mean and fails as a
generative model. Flow matching enables iterative denoising and realistic generation.
Time conditioning allows smooth transport from noise to data, while class conditioning
significantly improves convergence and controllability.
</p>
</div>

<hr>
<p><strong>End of CS180 Project 5 – Part B</strong></p>

</body>
</html>

