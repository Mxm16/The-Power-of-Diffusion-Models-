<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>CS180 Project 5 (Part A) — Diffusion Models</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
    }
    h1, h2, h3 { color: #1b3d8f; }
    figure { text-align: center; }
    figcaption { font-size: 14px; margin-top: 6px; color: #444; }
    .img-row {
      display: flex;
      gap: 20px;
      margin-top: 15px;
      margin-bottom: 35px;
    }
    .img-row img {
      width: 300px;
      border: 1px solid #ccc;
      border-radius: 6px;
    }
    .section { margin-bottom: 60px; }
  </style>
</head>

<body>

<h1>CS180: Introduction to Computer Vision<br>Project 5 (Part A): Diffusion Models</h1>

<hr>
<h2>Part 0 — Setup</h2>

<p><strong>Random Seed:</strong> 100</p>

<h3>Prompt Embeddings</h3>
<ul>
  <li>"an oil painting of a snowy mountain village"</li>
  <li>"a photo of a dog"</li>
  <li>"a rocket ship"</li>
</ul>

<h3>Generated Images</h3>
<p>Generated images at 20 and 50 inference steps demonstrate the effect of step size on image clarity.</p>
<div class="img-row">
  <figure><img src="img/prompt_snowyvillage.png" alt="Snowy Village 20 steps"><figcaption>Snowy Village (20 steps)</figcaption></figure>
  <figure><img src="img/prompt_snowyvillage1.png" alt="Snowy Village 50 steps"><figcaption>Snowy Village (50 steps)</figcaption></figure>
</div>

<p><strong>Reflection:</strong> Higher inference steps improve clarity and edge definition. Dog images are natural-looking, and rocket ship images benefit the most from increased steps.</p>

<hr>
<h2>Part 1.1 — Forward Process</h2>

<p>The forward process adds noise to an image according to the diffusion equation:</p>
<pre>
x_t = √(α̅_t) * x_0 + √(1 - α̅_t) * ε
</pre>
<p><strong>Thought Process:</strong> Gradually corrupt the image with Gaussian noise so the model can learn to recover x₀ from noisy inputs. Larger t corresponds to more noise.</p>

<div class="img-row">
  <figure><img src="img/camp_t250.png"><figcaption>t = 250</figcaption></figure>
  <figure><img src="img/camp_t500.png"><figcaption>t = 500</figcaption></figure>
  <figure><img src="img/camp_t750.png"><figcaption>t = 750</figcaption></figure>
</div>

<hr>
<h2>Part 1.2 — Gaussian Blur Denoising</h2>

<p>Applied classical Gaussian blur to noisy images:</p>
<pre>torchvision.transforms.functional.gaussian_blur()</pre>
<p><strong>Thought Process:</strong> High-frequency noise is reduced but image structure is not fully restored. This demonstrates why learned denoising is superior.</p>

<div class="img-row">
  <figure><img src="img/blur_250.png"><figcaption>Blurred (t=250)</figcaption></figure>
</div>

<hr>
<h2>Part 1.3 — One-Step Denoising</h2>

<p>Equation for one-step denoising:</p>
<pre>
x̂_0 = (x_t - √(1-α̅_t) * ε̂_θ(x_t, t, c)) / √(α̅_t)
</pre>
<p><strong>Thought Process:</strong> Predict the noise with the UNet and analytically solve for x₀. Works well for low noise levels (small t) but deteriorates with higher noise.</p>

<div class="img-row">
  <figure><img src="img/onestep_250.png"><figcaption>t = 250</figcaption></figure>
  <figure><img src="img/onestep_500.png"><figcaption>t = 500</figcaption></figure>
  <figure><img src="img/onestep_750.png"><figcaption>t = 750</figcaption></figure>
</div>

<hr>
<h2>Part 1.4 — Iterative Denoising</h2>

<p>Reverse diffusion equation used for iterative denoising:</p>
<pre>
x_{t-1} = 1/√(α_t) * (x_t - (1-α_t)/√(1-α̅_t) * ε̂_θ(x_t, t, c)) + σ_t * z
</pre>
<p><strong>Thought Process:</strong> At each timestep, remove predicted noise and optionally add small variance. Iterate from high t → 0 to recover clean image.</p>

<h3>Denoising Progress</h3>
<figure><img src="img/iter_0.png"><figcaption>Iterative Denoising Steps</figcaption></figure>

<hr>
<h2>Part 1.5 — Sampling from Pure Noise</h2>

<p>Set i_start = 0 and sample 5 images from random Gaussian noise:</p>
<p><strong>Thought Process:</strong> Use iterative denoising to generate images from scratch without any input. Quality is limited without CFG.</p>

<div class="img-row">
  <figure><img src="img/sample1.png"><figcaption>Sample 1</figcaption></figure>
  <figure><img src="img/sample2.png"><figcaption>Sample 2</figcaption></figure>
  <figure><img src="img/sample3.png"><figcaption>Sample 3</figcaption></figure>
  <figure><img src="img/sample4.png"><figcaption>Sample 4</figcaption></figure>
  <figure><img src="img/sample5.png"><figcaption>Sample 5</figcaption></figure>
</div>

<hr>
<h2>Part 1.6 — Classifier-Free Guidance (CFG)</h2>

<p>CFG equation:</p>
<pre>
ε̂ = ε̂_uncond + w * (ε̂_cond - ε̂_uncond)
</pre>
<p><strong>Thought Process:</strong> Compute unconditional and conditional noise estimates, amplify the conditional direction with guidance scale w to improve alignment with prompt.</p>

<div class="img-row">
  <figure><img src="img/cfg1.png"><figcaption>CFG Sample 1</figcaption></figure>
  <figure><img src="img/cfg2.png"><figcaption>CFG Sample 2</figcaption></figure>
  <figure><img src="img/cfg3.png"><figcaption>CFG Sample 3</figcaption></figure>
  <figure><img src="img/cfg4.png"><figcaption>CFG Sample 4</figcaption></figure>
  <figure><img src="img/cfg5.png"><figcaption>CFG Sample 5</figcaption></figure>
</div>

<hr>
<h2>Part 1.7 — SDEdit: Image-to-Image Projection</h2>

<p>Equation for SDEdit:</p>
<pre>
x_t = √(α̅_t) * x_0 + √(1 - α̅_t) * ε
Iteratively denoise with CFG to generate edited output
</pre>
<p><strong>Thought Process:</strong> Adding noise controls strength of edits. Low noise → minor changes, high noise → strong stylization guided by text prompt.</p>

<div class="img-row">
  <figure><img src="img/sdedit_1.png"><figcaption>Edited i_start 1-20</figcaption></figure>
  <figure><img src="img/sdedit_2.png"><figcaption>Edited i_start 1-20</figcaption></figure>
  <figure><img src="img/sdedit_3.png"><figcaption>Edited i_start 1-20</figcaption></figure>
</div>

<hr>
<h2>Part 1.7.1 — Editing Web & Hand-Drawn Images</h2>

<p><strong>Thought Process:</strong> Non-realistic inputs (sketches) can be projected onto the natural image manifold via CFG denoising for creative outputs.</p>
<div class="img-row">
  <figure><img src="img/web_1.png"><figcaption>Web Image Edit</figcaption></figure>
</div>
<div class="img-row">
  <figure><img src="img/draw1_1.png"><figcaption>Drawn Image 1</figcaption></figure>
  <figure><img src="img/draw1_20.png"><figcaption>Drawn Image 1 (i=20)</figcaption></figure>
  <figure><img src="img/draw2_1.png"><figcaption>Drawn Image 2</figcaption></figure>
  <figure><img src="img/draw2_20.png"><figcaption>Drawn Image 2 (i=20)</figcaption></figure>
</div>

<hr>
<h2>Part 1.7.2 — Inpainting</h2>

<p>Equation for inpainting:</p>
<pre>
x_{t-1}^{new} = mask * x_0 + (1-mask) * x_{t-1}^{predicted}
</pre>
<p><strong>Thought Process:</strong> Preserve unmasked pixels while generating content in masked regions using iterative denoising.</p>

<div class="img-row">
  <figure><img src="img/mask.png"><figcaption>Mask</figcaption></figure>
</div>
<div class="img-row">
  <figure><img src="img/inpaint1.png"><figcaption>Result 1</figcaption></figure>
  <figure><img src="img/inpaint2.png"><figcaption>Result 2</figcaption></figure>
  <figure><img src="img/inpaint3.png"><figcaption>Result 3</figcaption></figure>
</div>

<hr>
<h2>Part 1.7.3 — Text-Conditional Image-to-Image Translation</h2>
<p><strong>Thought Process:</strong> Use text prompt for stronger guidance in image editing. Output gradually aligns with prompt while preserving structure.</p>
<div class="img-row">
  <figure><img src="img/txt_1.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_2.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_3.png"><figcaption>i_start=1-20</figcaption></figure>
</div>

<hr>
<h2>Part 1.8 — Visual Anagrams</h2>

<p>Equation for visual anagrams:</p>
<pre>
ε̂ = (ε̂_prompt1 + flip(ε̂_prompt2)) / 2
x_{t-1} = reverse_step(x_t, ε̂)
</pre>
<p><strong>Thought Process:</strong> Average noise estimates from two prompts (one flipped) to create images that change appearance when rotated.</p>

<div class="img-row">
  <figure><img src="img/anagram1_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram1_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>
<div class="img-row">
  <figure><img src="img/anagram2_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram2_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>

<hr>
<h2>Part 1.9 — Hybrid Images</h2>

<p>Equation for hybrid images:</p>
<pre>
ε̂_hybrid = LowPass(ε̂1) + HighPass(ε̂2)
</pre>
<p><strong>Thought Process:</strong> Combine low-frequency structure of one image with high-frequency detail of another. Low-pass filter is Gaussian blur (kernel=33, sigma=2).</p>

<div class="img-row">
  <figure><img src="img/hybrid1.png"><figcaption>Hybrid 1</figcaption></figure>
  <figure><img src="img/hybrid2.png"><figcaption>Hybrid 2</figcaption></figure>
</div>

<hr>
<p>End of CS180 Project 5A</p>
<hr>


<!-- ================================================================ -->
<h1>Project 5 (Part B): Flow Matching from Scratch</h1>

<!-- ================================================================ -->
<div class="section">
<h2>Overview</h2>
<p>
In this part, we train flow-matching generative models from scratch on the MNIST dataset.
We first explore the limitations of single-step denoising, and then introduce flow
matching with time and class conditioning to enable iterative image generation.
</p>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1 — Training a Single-Step Denoising UNet</h2>

<p>
We begin by training a simple one-step denoiser. Given a noisy MNIST image, the goal is
to train a UNet that directly maps it to a clean image using an L2 loss.
</p>

<!-- ======================= 1.1 ======================= -->
<h3>1.1 Implementing the UNet</h3>
<p>
The denoiser is implemented as an unconditional UNet consisting of downsampling and
upsampling blocks with skip connections. The network preserves spatial structure while
progressively transforming feature representations.
</p>

<!-- ======================= 1.2 ======================= -->
<h3>1.2 Using the UNet to Train a Denoiser</h3>

<p>
Training pairs are generated by adding Gaussian noise to clean MNIST digits.
Given a clean image x and noise level σ, the noisy image is constructed as:
</p>

<pre>
x_noisy = x + σ · ε,   ε ~ N(0, I)
</pre>
<figure>
  <img src="img/part1_noise.png">
  <figcaption>
    Visualization of the noising process for noise levels
    [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
  </figcaption>
</figure>

<h4>1.2.1 Training</h4>
<p>
<b>Implementation idea:</b>  
For each batch, sample clean images and generate new noisy images on-the-fly using a random σ. 
Feed the noisy batch through the UNet, compute L2 loss with respect to the clean images, 
and backpropagate to update weights. Repeat for multiple epochs so the model learns to 
denoise a range of noise levels.
</p>

<figure>
  <img src="img/part1_loss.png">
  <figcaption>Training loss curve for single-step denoising UNet</figcaption>
</figure>

<figure>
  <img src="img/part1_epoch1.png">
  <figcaption>Sample denoising results after Epoch 1 and Epoch 5</figcaption>
</figure>

<h4>1.2.2 Out-of-Distribution Testing</h4>
<p>
<b>Implementation idea:</b>  
After training, feed noisy images with σ values not seen during training through the trained UNet. 
This tests whether the network generalizes to noise levels outside the training distribution.
</p>

<figure>
  <img src="img/part1_ood.png">
  <figcaption>Denoising results on unseen noise levels</figcaption>
</figure>

<h4>1.2.3 Denoising Pure Noise</h4>
<p>
<b>Implementation idea:</b>  
Replace clean inputs with pure Gaussian noise. Train the UNet to predict the expected clean image from noise. 
This simulates unconditional generation. Since MSE minimizes squared distance, the model will converge 
to the mean of all training images, explaining why outputs are blurry and similar.
</p>

<div class="img-row">
  <figure><img src="img/part1_pure_epoch1.png"><figcaption>Epoch 1</figcaption></figure>
  <figure><img src="img/part1_pure_epoch5.png"><figcaption>Epoch 5</figcaption></figure>
</div>

<figure>
  <img src="img/part1_loss1.png">
  <figcaption>Training loss while denoising pure noise</figcaption>
</figure>

<pre>
y* = argmin_y E[(y - x_digit)^2]
</pre>

</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 2 — Training a Flow Matching Model</h2>

<p>
Single-step denoising is insufficient for generation. We instead use flow matching,
which enables iterative denoising by learning a continuous vector field that transports
noise to the data distribution.
</p>

<h3>2.1 Time-Conditioned UNet</h3>
<p>
<b>Implementation idea:</b>  
Introduce scalar timestep t ∈ [0,1] and embed it via FCBlocks into intermediate layers. 
This lets the UNet learn a flow that depends on the current timestep, enabling smooth interpolation 
from noise to clean images.
</p>

<h3>2.2 Training the Time-Conditioned UNet</h3>
<p>
<b>Implementation idea:</b>  
For each batch: sample a clean image x and a random timestep t.  
Construct the noisy input x_t by interpolating between x and noise.  
Predict the flow field v(x_t, t) with the UNet.  
Compute L2 loss between predicted flow and true flow (x - x_t)/dt.  
Backpropagate and update the UNet weights. Repeat for all batches across multiple epochs.
</p>

<figure>
  <img src="img/part2_time_loss.png">
  <figcaption>Training loss curve for time-conditioned UNet</figcaption>
</figure>

<h3>2.3 Sampling from the Time-Conditioned UNet</h3>
<p>
<b>Implementation idea:</b>  
Start from pure Gaussian noise. At each timestep, feed current image and timestep into the UNet to predict the flow.  
Update the image along the flow direction. Iterate through all timesteps from 0 to 1.  
As training progresses, the network gradually transforms noise into recognizable digits.
</p>

<div class="img-row">
  <figure><img src="img/part2_time_epoch1.png"><figcaption>Epoch 1</figcaption></figure>
  <figure><img src="img/part2_time_epoch5.png"><figcaption>Epoch 5</figcaption></figure>
  <figure><img src="img/part2_time_epoch10.png"><figcaption>Epoch 10</figcaption></figure>
</div>

<h3>2.4 Adding Class Conditioning</h3>
<p>
<b>Implementation idea:</b>  
Introduce a one-hot class vector c for digit labels. Embed c via FCBlocks and inject into UNet layers.  
Randomly drop c 10% of the time for classifier-free guidance, enabling both conditional and unconditional generation.
</p>

<h3>2.5 Training the Class-Conditioned UNet</h3>
<p>
<b>Implementation idea:</b>  
Same as time-conditioned training, but concatenate or add the class-conditioning embedding to intermediate layers.  
Loss remains L2 between predicted and true flow. Random dropout of class embeddings ensures the model can also generate unconditionally.
</p>

<figure>
  <img src="img/part2_class_loss.png">
  <figcaption>Training loss curve for class-conditioned UNet</figcaption>
</figure>

<h3>2.6 Sampling with Class Conditioning</h3>
<p>
<b>Implementation idea:</b>  
Start from noise. At each timestep, provide both the current image and the one-hot class vector to the UNet to predict flow.  
Update the image along the flow. Repeat for all timesteps to generate digit-specific images.  
Use classifier-free guidance to improve sample quality.
</p>

<figure>
  <img src="img/part2_class_epoch1.png">
  <img src="img/part2_class_epoch5.png">
  <img src="img/part2_class_epoch10.png">
  <figcaption>
    Class-conditioned samples.
    Each row corresponds to one digit (0–9) with four samples per class.
  </figcaption>
</figure>

<h3>Removing the Learning Rate Scheduler</h3>
<p>
<b>Implementation idea:</b>  
Removed the exponential decay scheduler for simplicity.  
Compensate by lowering the fixed learning rate (1e-2 → 1e-3) and monitor training.  
Training remains stable and convergence is similar.
</p>

<figure>
  <img src="img/part2_no_sched_loss.png">
  <figcaption>Training loss without a learning rate scheduler</figcaption>
</figure>

<figure>
  <img src="img/part2_no_sched_epoch1.png">
  <img src="img/part2_no_sched_epoch5.png">
  <img src="img/part2_no_sched_epoch10.png">
  <figcaption>
    Class-conditioned samples after training without a learning rate scheduler.
  </figcaption>
</figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Conclusion</h2>

<p>
We showed that single-step denoising collapses to the dataset mean and fails as a
generative model. Flow matching enables iterative denoising and realistic generation.
Time conditioning allows smooth transport from noise to data, while class conditioning
significantly improves convergence and controllability.
</p>
</div>

<hr>
<p><strong>End of CS180 Project 5 – Part B</strong></p>

</body>
</html>

