<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>CS180 Project 5 (Part A) — Diffusion Models</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
    }
    h1, h2, h3 { color: #1b3d8f; }
    figure { text-align: center; }
    figcaption { font-size: 14px; margin-top: 6px; color: #444; }
    .img-row {
      display: flex;
      gap: 20px;
      margin-top: 15px;
      margin-bottom: 35px;
    }
    .img-row img {
      width: 300px;
      border: 1px solid #ccc;
      border-radius: 6px;
    }
    .section { margin-bottom: 60px; }
  </style>
</head>

<body>

<h1>CS180: Introduction to Computer Vision<br>Project 5 (Part A): Diffusion Models</h1>

<hr>

<!-- ================================================================ -->
<div class="section">
<h2>Part 0 — Setup</h2>

<h3>Prompt Embeddings I Generated</h3>
<p>For this project, I generated embeddings for the following prompts:</p>

<ul>
  <li><strong>"an oil painting of a snowy mountain village"</strong></li>
  <li><strong>"a photo of a dog"</strong></li>
  <li><strong>"a rocket ship"</strong></li>
</ul>

<h3>Generated Images from My Prompts</h3>
<p>Below are sampled images using the above three prompts:</p>

<div class="img-row">
  <figure><img src="img/prompt_snowyvillage.png"><figcaption>Snowy Mountain Village</figcaption></figure>
  <figure><img src="img/prompt_dog.png"><figcaption>Photo of a Dog</figcaption></figure>
  <figure><img src="img/prompt_rocket.png"><figcaption>Rocket Ship</figcaption></figure>
</div>

<p><strong>Reflection:</strong> The model captures general structure for all three prompts.  
The snowy mountain village outputs exhibit clear oil painting texture.  
Dog generations are varied and reasonably naturalistic.  
Rocket ship generation quality depends strongly on inference steps.</p>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.1 — Forward Process</h2>

<p>I implemented the forward process:</p>

<pre>NoisyImage_t = √(α̅_t) * image + √(1 - α̅_t) * noise</pre>

<p>Below are noisy Campanile images at various timesteps:</p>

<div class="img-row">
  <figure><img src="img/camp_t250.png"><figcaption>t = 250</figcaption></figure>
  <figure><img src="img/camp_t500.png"><figcaption>t = 500</figcaption></figure>
  <figure><img src="img/camp_t750.png"><figcaption>t = 750</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.2 — Gaussian Blur Denoising</h2>

<p>I applied <code>torchvision.transforms.functional.gaussian_blur</code> to the noisy images.
Gaussian blur reduces high-frequency noise but fails to restore structure.</p>

<div class="img-row">
  <figure><img src="img/blur_250.png"><figcaption>Blurred (t=250)</figcaption></figure>
  <figure><img src="img/blur_500.png"><figcaption>Blurred (t=500)</figcaption></figure>
  <figure><img src="img/blur_750.png"><figcaption>Blurred (t=750)</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.3 — One-Step Denoising</h2>

<p>I passed the noisy images through <code>stage_1.unet</code> and removed noise using equation A.2.  
The model produces reasonable reconstructions for low-noise inputs, but performance degrades at t=750.</p>

<div class="img-row">
  <figure><img src="img/onestep_250.png"><figcaption>One-Step (t=250)</figcaption></figure>
  <figure><img src="img/onestep_500.png"><figcaption>One-Step (t=500)</figcaption></figure>
  <figure><img src="img/onestep_750.png"><figcaption>One-Step (t=750)</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.4 — Iterative Denoising</h2>

<p>I constructed a strided timestep schedule:</p>
<pre>[990, 960, 930, ..., 0]</pre>

<p>My iterative denoising implementation uses the reverse diffusion formula and <code>add_variance</code> as required.</p>

<h3>Denoising Progress</h3>
<div class="img-row">
  <figure><img src="img/iter_0.png"><figcaption>>Denoising Progress</figcaption></figure>
</div>


<!-- ================================================================ -->
<div class="section">
<h2>Part 1.5 — Sampling from Pure Noise</h2>
<p>Using <code>i_start = 0</code>, I sampled 5 images with the prompt “a high quality photo”.</p>

<div class="img-row">
  <figure><img src="img/sample1.png"><figcaption>Sample 1</figcaption></figure>
  <figure><img src="img/sample2.png"><figcaption>Sample 2</figcaption></figure>
  <figure><img src="img/sample3.png"><figcaption>Sample 3</figcaption></figure>
</div>

<div class="img-row">
  <figure><img src="img/sample4.png"><figcaption>Sample 4</figcaption></figure>
  <figure><img src="img/sample5.png"><figcaption>Sample 5</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.6 — Classifier-Free Guidance</h2>

<p>I implemented CFG using conditional and unconditional UNet predictions.  
CFG scale = 7 significantly improves sampling quality.</p>

<div class="img-row">
  <figure><img src="img/cfg1.png"><figcaption>CFG Sample 1</figcaption></figure>
  <figure><img src="img/cfg2.png"><figcaption>CFG Sample 2</figcaption></figure>
  <figure><img src="img/cfg3.png"><figcaption>CFG Sample 3</figcaption></figure>
  <figure><img src="img/cfg4.png"><figcaption>CFG Sample 4</figcaption></figure>
  <figure><img src="img/cfg5.png"><figcaption>CFG Sample 5</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7 — SDEdit: Image-to-Image Projection</h2>

<p>I used the prompt <strong>"an oil painting of a snowy mountain village"</strong> for text-guided SDEdit.  
Edits grow stronger as noise increases.</p>

<div class="img-row">
  <figure><img src="img/sdedit_1.png"><figcaption>i_start=1</figcaption></figure>
  <figure><img src="img/sdedit_3.png"><figcaption>i_start=3</figcaption></figure>
  <figure><img src="img/sdedit_5.png"><figcaption>i_start=5</figcaption></figure>
</div>

<div class="img-row">
  <figure><img src="img/sdedit_7.png"><figcaption>i_start=7</figcaption></figure>
  <figure><img src="img/sdedit_10.png"><figcaption>i_start=10</figcaption></figure>
  <figure><img src="img/sdedit_20.png"><figcaption>i_start=20</figcaption></figure>
</div>

<h3>Two Additional Images</h3>
<p>(Insert images here)</p>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.1 — Editing Web Images & Hand-Drawn Images</h2>

<h3>Web Image Example</h3>
<div class="img-row">
  <figure><img src="img/web_1.png"><figcaption>i_start=1-20</figcaption></figure>
</div>

<h3>Hand-Drawn Examples</h3>
<div class="img-row">
  <figure><img src="img/draw1_1.png"><figcaption>Drawn Image 1 (i=1-20)</figcaption></figure>
  <figure><img src="img/draw1_20.png"><figcaption>Drawn Image</figcaption></figure>
</div>

<div class="img-row">
  <figure><img src="img/draw2_1.png"><figcaption>Drawn Image 2 (i=1-20)</figcaption></figure>
  <figure><img src="img/draw2_20.png"><figcaption>Drawn Image(i=20)</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.2 — Inpainting</h2>

<p>I implemented inpainting by overwriting unmasked pixels with noised versions of the original image at every step.</p>

<h3>Mask</h3>
<figure><img src="img/mask.png"><figcaption>Mask</figcaption></figure>

<h3>Inpainting Results</h3>
<div class="img-row">
  <figure><img src="img/inpaint1.png"><figcaption>Result 1</figcaption></figure>
  <figure><img src="img/inpaint2.png"><figcaption>Result 2</figcaption></figure>
  <figure><img src="img/inpaint3.png"><figcaption>Result 3</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.3 — Text-Conditional Image-to-Image Translation</h2>

<p>I used the prompt <strong>"an oil painting of a snowy mountain village"</strong> for text-guided projection.</p>

<div class="img-row">
  <figure><img src="img/txt_1.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_2.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_3.png"><figcaption>i_start=1-20</figcaption></figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.8 — Visual Anagrams</h2>

<p>I implemented visual anagrams using two prompts:</p>

<ul>
  <li><strong>"an oil painting of an old man"</strong> (upright)</li>
  <li><strong>"an oil painting of people around a campfire"</strong> (flipped)</li>
</ul>

<div class="img-row">
  <figure><img src="img/anagram1_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram1_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>

<ul>
  <li><strong>"a pencil"</strong> (upright)</li>
  <li><strong>"a rocket ship"</strong> (flipped)</li>
</ul>
  
<div class="img-row">
  <figure><img src="img/anagram2_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram2_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.9 — Hybrid Images</h2>

<p>I implemented hybrid images using:</p>
<pre>ε = lowpass(ε₁) + highpass(ε₂)</pre>

<p>The low-pass filter is Gaussian blur (kernel_size=33, sigma=2).</p>

<h3>Hybrid Image 1</h3>
<div class="img-row">
  <figure><img src="img/hybrid1.png"><figcaption>Hybrid 1</figcaption></figure>
</div>

<h3>Hybrid Image 2</h3>
<div class="img-row">
  <figure><img src="img/hybrid2.png"><figcaption>Hybrid 2</figcaption></figure>
</div>

</div>

<hr>
<p>End of CS180 Project 5A</p>

</body>
</html>

