<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>CS180 Project 5 (Part A) — Diffusion Models</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
    }
    h1, h2, h3 { color: #1b3d8f; }
    figure { text-align: center; }
    figcaption { font-size: 14px; margin-top: 6px; color: #444; }
    .img-row {
      display: flex;
      gap: 20px;
      margin-top: 15px;
      margin-bottom: 35px;
    }
    .img-row img {
      width: 300px;
      border: 1px solid #ccc;
      border-radius: 6px;
    }
    .section { margin-bottom: 60px; }
  </style>
</head>

<body>

<h1>CS180: Introduction to Computer Vision<br>Project 5 (Part A): Diffusion Models</h1>

<hr>

<!-- ================================================================ -->
<h2>Part 0 — Setup</h2>

<h3>Random Seed</h3>
<p>The random seed used for all image generations is <strong>100</strong>.</p>

<h3>Prompt Embeddings I Generated</h3>
<p>I generated embeddings for the following prompts:</p>
<ul>
  <li><strong>"an oil painting of a snowy mountain village"</strong></li>
  <li><strong>"a photo of a dog"</strong></li>
  <li><strong>"a rocket ship"</strong></li>
</ul>

<h3>Generated Images (20 Inference Steps)</h3>
<div class="img-row">
  <figure><img src="img/prompt_snowyvillage.png"><figcaption>Snowy Mountain Village</figcaption></figure>
  <figure><img src="img/prompt_dog.png"><figcaption>Photo of a Dog</figcaption></figure>
  <figure><img src="img/prompt_rocket.png"><figcaption>Rocket Ship</figcaption></figure>
</div>

<h3>Generated Images (50 Inference Steps)</h3>
<div class="img-row">
  <figure><img src="img/prompt_snowyvillage1.png"><figcaption>Snowy Mountain Village</figcaption></figure>
  <figure><img src="img/prompt_dog1.png"><figcaption>Photo of a Dog</figcaption></figure>
  <figure><img src="img/prompt_rocket1.png"><figcaption>Rocket Ship</figcaption></figure>
</div>

<h3>Reflection</h3>
<p>
The model produced recognizable samples for all three prompts.  
The snowy mountain village images consistently captured an oil-painting texture.  
The dog images were natural-looking, though lower inference steps resulted in slightly more noise.  
The rocket ship prompt showed the clearest difference:  
20 inference steps produced blurrier edges and artifacts, while 50 steps resulted in cleaner structure and better-defined shapes.  
This confirms that higher inference steps significantly improve coherence and detail.
</p>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.1 — Forward Process</h2>

<p>I implemented the forward process:</p>

<pre>NoisyImage_t = √(α̅_t) * image + √(1 - α̅_t) * noise</pre>

<p>Below are noisy Campanile images at various timesteps:</p>

<div class="img-row">
  <figure><img src="img/camp_t250.png"><figcaption>t = 250</figcaption></figure>
  <figure><img src="img/camp_t500.png"><figcaption>t = 500</figcaption></figure>
  <figure><img src="img/camp_t750.png"><figcaption>t = 750</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.2 — Gaussian Blur Denoising</h2>

<p>I applied <code>torchvision.transforms.functional.gaussian_blur</code> to the noisy images.
Gaussian blur reduces high-frequency noise but fails to restore structure.</p>

<figure><img src="img/blur_250.png"><figcaption>Blurred (t=250)</figcaption></figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.3 — One-Step Denoising</h2>

<p>I passed the noisy images through <code>stage_1.unet</code> and removed noise using equation A.2.  
The model produces reasonable reconstructions for low-noise inputs, but performance degrades at t=750.</p>

<div class="img-row">
  <figure><img src="img/onestep_250.png"><figcaption>One-Step (t=250)</figcaption></figure>
  <figure><img src="img/onestep_500.png"><figcaption>One-Step (t=500)</figcaption></figure>
  <figure><img src="img/onestep_750.png"><figcaption>One-Step (t=750)</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.4 — Iterative Denoising</h2>

<p>I constructed a strided timestep schedule:</p>
<pre>[990, 960, 930, ..., 0]</pre>

<p>My iterative denoising implementation uses the reverse diffusion formula and <code>add_variance</code> as required.</p>

<h3>Denoising Progress</h3>
  <figure><img src="img/iter_0.png"><figcaption>>Denoising Progress</figcaption></figure>



<!-- ================================================================ -->
<div class="section">
<h2>Part 1.5 — Sampling from Pure Noise</h2>
<p>Using <code>i_start = 0</code>, I sampled 5 images with the prompt “a high quality photo”.</p>

<div class="img-row">
  <figure><img src="img/sample1.png"><figcaption>Sample 1</figcaption></figure>
  <figure><img src="img/sample2.png"><figcaption>Sample 2</figcaption></figure>
  <figure><img src="img/sample3.png"><figcaption>Sample 3</figcaption></figure>
</div>

<div class="img-row">
  <figure><img src="img/sample4.png"><figcaption>Sample 4</figcaption></figure>
  <figure><img src="img/sample5.png"><figcaption>Sample 5</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.6 — Classifier-Free Guidance</h2>

<p>I implemented CFG using conditional and unconditional UNet predictions.  
CFG scale = 7 significantly improves sampling quality.</p>

<div class="img-row">
  <figure><img src="img/cfg1.png"><figcaption>CFG Sample 1</figcaption></figure>
  <figure><img src="img/cfg2.png"><figcaption>CFG Sample 2</figcaption></figure>
  <figure><img src="img/cfg3.png"><figcaption>CFG Sample 3</figcaption></figure>
</div>
<div class="img-row">
  <figure><img src="img/cfg4.png"><figcaption>CFG Sample 4</figcaption></figure>
  <figure><img src="img/cfg5.png"><figcaption>CFG Sample 5</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7 — SDEdit: Image-to-Image Projection</h2>

<p>I used the prompt <strong>"an oil painting of a snowy mountain village"</strong> for text-guided SDEdit.  
Edits grow stronger as noise increases.</p>

  <figure><img src="img/sdedit_1.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/sdedit_2.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/sdedit_3.png"><figcaption>i_start=1-20</figcaption></figure>



<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.1 — Editing Web Images & Hand-Drawn Images</h2>

<h3>Web Image Example</h3>

  <figure><img src="img/web_1.png"><figcaption>i_start=1-20</figcaption></figure>


<h3>Hand-Drawn Examples</h3>

  <figure><img src="img/draw1_1.png"><figcaption>Drawn Image 1 (i=1-20)</figcaption></figure>
  <figure><img src="img/draw1_20.png"><figcaption>Drawn Image</figcaption></figure>



  <figure><img src="img/draw2_1.png"><figcaption>Drawn Image 2 (i=1-20)</figcaption></figure>
  <figure><img src="img/draw2_20.png"><figcaption>Drawn Image(i=20)</figcaption></figure>

</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.2 — Inpainting</h2>

<p>I implemented inpainting by overwriting unmasked pixels with noised versions of the original image at every step.</p>

<h3>Mask</h3>
<figure><img src="img/mask.png"><figcaption>Mask</figcaption></figure>

<h3>Inpainting Results</h3>
<div class="img-row">
  <figure><img src="img/inpaint1.png"><figcaption>Result 1</figcaption></figure>
  <figure><img src="img/inpaint2.png"><figcaption>Result 2</figcaption></figure>
  <figure><img src="img/inpaint3.png"><figcaption>Result 3</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.7.3 — Text-Conditional Image-to-Image Translation</h2>

<p>I used the prompt <strong>"an oil painting of a snowy mountain village"</strong> for text-guided projection.</p>

<div class="img-row">
  <figure><img src="img/txt_1.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_2.png"><figcaption>i_start=1-20</figcaption></figure>
  <figure><img src="img/txt_3.png"><figcaption>i_start=1-20</figcaption></figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.8 — Visual Anagrams</h2>

<p>I implemented visual anagrams using two prompts:</p>

<ul>
  <li><strong>"an oil painting of an old man"</strong> (upright)</li>
  <li><strong>"an oil painting of people around a campfire"</strong> (flipped)</li>
</ul>

<div class="img-row">
  <figure><img src="img/anagram1_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram1_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>

<ul>
  <li><strong>"a pencil"</strong> (upright)</li>
  <li><strong>"a rocket ship"</strong> (flipped)</li>
</ul>
  
<div class="img-row">
  <figure><img src="img/anagram2_upright.png"><figcaption>Upright</figcaption></figure>
  <figure><img src="img/anagram2_flipped.png"><figcaption>Flipped</figcaption></figure>
</div>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1.9 — Hybrid Images</h2>

<p>I implemented hybrid images using:</p>
<pre>ε = lowpass(ε₁) + highpass(ε₂)</pre>

<p>The low-pass filter is Gaussian blur (kernel_size=33, sigma=2).</p>

<h3>Hybrid Image 1</h3>
<div class="img-row">
  <figure><img src="img/hybrid1.png"><figcaption>Hybrid 1</figcaption></figure>
</div>

<h3>Hybrid Image 2</h3>
<div class="img-row">
  <figure><img src="img/hybrid2.png"><figcaption>Hybrid 2</figcaption></figure>
</div>

</div>

<hr>
<p>End of CS180 Project 5A</p>

<hr>

<!-- ================================================================ -->
<h1>Project 5 (Part B): Flow Matching from Scratch</h1>

<!-- ================================================================ -->
<div class="section">
<h2>Overview</h2>
<p>
In this part, we train flow-matching generative models from scratch on the MNIST dataset.
We first explore the limitations of single-step denoising, and then introduce flow
matching with time and class conditioning to enable iterative image generation.
</p>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 1 — Training a Single-Step Denoising UNet</h2>

<p>
We begin by training a simple one-step denoiser. Given a noisy MNIST image, the goal is
to train a UNet that directly maps it to a clean image using an L2 loss.
</p>

<!-- ======================= 1.1 ======================= -->
<h3>1.1 Implementing the UNet</h3>
<p>
The denoiser is implemented as an unconditional UNet consisting of downsampling and
upsampling blocks with skip connections. The network preserves spatial structure while
progressively transforming feature representations.
</p>

<!-- ======================= 1.2 ======================= -->
<h3>1.2 Using the UNet to Train a Denoiser</h3>

<p>
Training pairs are generated by adding Gaussian noise to clean MNIST digits.
Given a clean image x and noise level σ, the noisy image is constructed as:
</p>

<pre>
x_noisy = x + σ · ε,   ε ~ N(0, I)
</pre>
<figure>
  <img src="img/part1_noise.png">
  <figcaption>
    Visualization of the noising process for noise levels
    [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
  </figcaption>
</figure>
<!-- ======================= 1.2.1 ======================= -->
<h4>1.2.1 Training</h4>

<p>
The UNet is trained to denoise images corrupted by randomly sampled noise levels.
Noise is added on-the-fly for each batch, improving robustness and generalization.
</p>

<figure>
  <img src="img/part1_loss.png">
  <figcaption>Training loss curve for single-step denoising UNet</figcaption>
</figure>

<figure>
  <img src="img/part1_epoch1.png">
  <figcaption>Sample denoising results after Epoch 1 and Epoch 5</figcaption>
</figure>

<!-- ======================= 1.2.2 ======================= -->
<h4>1.2.2 Out-of-Distribution Testing</h4>

<p>
We evaluate the trained denoiser on noise levels not seen during training to assess
its ability to generalize beyond the training noise distribution.
</p>

<figure>
  <img src="img/part1_ood.png">
  <figcaption>Denoising results on unseen noise levels</figcaption>
</figure>

<!-- ======================= 1.2.3 ======================= -->
<h4>1.2.3 Denoising Pure Noise</h4>

<p>
To turn denoising into a generative task, we train the UNet to denoise pure Gaussian noise.
This setting removes all information about the underlying digit and tests whether the
model can perform unconditional generation.
</p>

<div class="img-row">
  <figure><img src="img/part1_pure_epoch1.png"><figcaption>Epoch 1</figcaption></figure>
  <figure><img src="img/part1_pure_epoch5.png"><figcaption>Epoch 5</figcaption></figure>
</div>

<figure>
  <img src="img/part1_loss1.png">
  <figcaption>Training loss while denoising pure noise</figcaption>
</figure>

<p>
When trained with an MSE loss on pure noise, the network converges to predicting the
average of the training data distribution. Since the input contains no information
about which digit should be generated, the optimal solution under an L2 objective is:
</p>

<pre>
y* = argmin_y E[(y - x_digit)^2]
</pre>

<p>
The solution corresponds to the mean (centroid) of all MNIST digits. Consequently,
the generated outputs appear as blurry, averaged digit-like patterns rather than
distinct digits. All samples look nearly identical, explaining why single-step
denoising fails as a generative model.
</p>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Part 2 — Training a Flow Matching Model</h2>

<p>
Single-step denoising is insufficient for generation. We instead use flow matching,
which enables iterative denoising by learning a continuous vector field that transports
noise to the data distribution.
</p>

<!-- ======================= 2.1 ======================= -->
<h3>2.1 Time-Conditioned UNet</h3>

<p>
We extend the UNet by conditioning it on a continuous timestep t ∈ [0,1].
Fully-connected conditioning blocks (FCBlocks) inject the timestep into intermediate
layers, allowing the model to learn a time-dependent flow field.
</p>

<!-- ======================= 2.2 ======================= -->
<h3>2.2 Training the Time-Conditioned UNet</h3>

<p>
During training, we sample a clean image x, a random timestep t, and construct an
interpolated noisy image. The UNet is trained to predict the flow field that moves
the sample toward the clean data distribution.
</p>

<figure>
  <img src="img/part2_time_loss.png">
  <figcaption>Training loss curve for time-conditioned UNet</figcaption>
</figure>

<!-- ======================= 2.3 ======================= -->
<h3>2.3 Sampling from the Time-Conditioned UNet</h3>

<p>
Sampling is performed by iteratively updating a noisy image using the predicted flow.
As training progresses, digit-like structures gradually emerge.
</p>

<div class="img-row">
  <figure><img src="img/part2_time_epoch1.png"><figcaption>Epoch 1</figcaption></figure>
  <figure><img src="img/part2_time_epoch5.png"><figcaption>Epoch 5</figcaption></figure>
  <figure><img src="img/part2_time_epoch10.png"><figcaption>Epoch 10</figcaption></figure>
</div>

<!-- ======================= 2.4 ======================= -->
<h3>2.4 Adding Class Conditioning</h3>

<p>
To improve generation quality and controllability, we further condition the UNet on
digit class labels using one-hot encodings. Classifier-free guidance is applied by
randomly dropping the class-conditioning signal during training.
</p>

<!-- ======================= 2.5 ======================= -->
<h3>2.5 Training the Class-Conditioned UNet</h3>

<figure>
  <img src="img/part2_class_loss.png">
  <figcaption>Training loss curve for class-conditioned UNet</figcaption>
</figure>

<!-- ======================= 2.6 ======================= -->
<h3>2.6 Sampling with Class Conditioning</h3>

<figure>
  <img src="img/part2_class_epoch1.png">
  <img src="img/part2_class_epoch5.png">
  <img src="img/part2_class_epoch10.png">
  <figcaption>
    Class-conditioned samples.
    Each row corresponds to one digit (0–9) with four samples per class.
  </figcaption>
</figure>

<!-- ======================= Scheduler ======================= -->
<h3>Removing the Learning Rate Scheduler</h3>

<p>
To simplify training, we removed the exponential learning rate scheduler and compensated
by reducing the fixed learning rate from <code>1e-2</code> to <code>1e-3</code>.
Training remained stable and achieved comparable performance.
</p>

<figure>
  <img src="img/part2_no_sched_loss.png">
  <figcaption>Training loss without a learning rate scheduler</figcaption>
</figure>

<figure>
  <img src="img/part2_no_sched_epoch1.png">
  <img src="img/part2_no_sched_epoch5.png">
  <img src="img/part2_no_sched_epoch10.png">
  <figcaption>
    Class-conditioned samples after training without a learning rate scheduler.
  </figcaption>
</figure>
</div>

<!-- ================================================================ -->
<div class="section">
<h2>Conclusion</h2>

<p>
We showed that single-step denoising collapses to the dataset mean and fails as a
generative model. Flow matching enables iterative denoising and realistic generation.
Time conditioning allows smooth transport from noise to data, while class conditioning
significantly improves convergence and controllability.
</p>
</div>

<hr>
<p><strong>End of CS180 Project 5 – Part B</strong></p>


</body>
</html>

